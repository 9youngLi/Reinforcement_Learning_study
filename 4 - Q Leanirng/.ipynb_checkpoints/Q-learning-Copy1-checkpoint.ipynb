{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "#Jupyter notebook 에서 pop up window\n",
    "%matplotlib tk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open AI Gym을 이용한 Custom Environment 생성 예\n",
    "class GridEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 초기 환경 구성 시 필요한 파라미터 설정\n",
    "        self.map_size = (100,100)\n",
    "        self.agent_pos = [0,0]\n",
    "        self.obstacle = []\n",
    "                        \n",
    "        self.goal = [0,0]#[self.map_size[0]-1,self.map_size[1]-1]\n",
    "        self.is_done = None\n",
    "        # Temporal Difference \n",
    "        self.V = np.zeros([self.map_size[0],self.map_size[1]])\n",
    "        self.Q = np.zeros([4,self.map_size[0],self.map_size[1]])\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.1\n",
    "        self.epsillon = 1\n",
    "       \n",
    "        # Open AI gym 환경 정보 설정\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.obs_space = gym.spaces.Discrete(3)\n",
    "        \n",
    "        #plt figtext 위치\n",
    "        self.text_pos_x = 0.8\n",
    "        self.text_pos_y = 0.9\n",
    "\n",
    "    def step(self, action):\n",
    "        self.prev_state = copy.deepcopy(self.agent_pos)\n",
    "        if action == 0: #Uo\n",
    "            self.agent_pos[0] += -1\n",
    "        elif action == 1: #Down\n",
    "            self.agent_pos[0] += +1\n",
    "        elif action == 2: #Left\n",
    "            self.agent_pos[1] += -1\n",
    "        elif action == 3: #Right\n",
    "            self.agent_pos[1] += +1\n",
    "        else:\n",
    "            raise Exception(\"Action is not defined\")\n",
    "      \n",
    "    \n",
    "        self.next_state = copy.deepcopy(self.agent_pos)\n",
    "        \n",
    "        #self.render_text(self.obs(),self.get_reward())\n",
    "            \n",
    "        return self.obs(), self.get_reward(), self._is_done(), self.next_state, self.prev_state\n",
    "    \n",
    "    def obs(self):\n",
    "        \n",
    "        if self.agent_pos in self.obstacle:\n",
    "            return 0\n",
    "        \n",
    "        elif self.agent_pos == self.goal:    \n",
    "            return 1\n",
    "        elif self.agent_pos[0] < 0 \\\n",
    "        or self.agent_pos[1] < 0 \\\n",
    "        or self.agent_pos[0] > self.map_size[0]-1 \\\n",
    "        or self.agent_pos[1] > self.map_size[1]-1:\n",
    "            return 3\n",
    "        else:    \n",
    "            return 2\n",
    "    \n",
    "    def _is_done(self):\n",
    "        # 맵 밖으로 나갔을 시 \n",
    "        if self.agent_pos[0] < 0 \\\n",
    "        or self.agent_pos[1] < 0 \\\n",
    "        or self.agent_pos[0] > self.map_size[0]-1 \\\n",
    "        or self.agent_pos[1] > self.map_size[1]-1:\n",
    "            return True\n",
    "        # 도착 지점 도착시\n",
    "        elif self.agent_pos == self.goal:\n",
    "            return True\n",
    "        \n",
    "        else:\n",
    "            return False\n",
    "            \n",
    "    def reset(self):\n",
    "        '''환경 초기화'''\n",
    "        \n",
    "        #맵 사이즈 설정\n",
    "        self.world = np.zeros(self.map_size)        \n",
    "        \n",
    "        #에이전트의 초기위치 및 장애물 위치 설정\n",
    "        for obs_x,obs_y in self.obstacle:\n",
    "            self.world[obs_x,obs_y] = 2\n",
    "        \n",
    "        \n",
    "        self.agent_pos = [self.map_size[0]-1,self.map_size[1]-1]#[np.random.randint(self.map_size[0]),np.random.randint(self.map_size[1])]\n",
    "\n",
    "        return self.obs(),self.get_reward()\n",
    "    \n",
    "    def policy(self, state = None): \n",
    "        if state[0] < 0 \\\n",
    "        or state[1] < 0 \\\n",
    "        or state[0] > self.map_size[0]-1 \\\n",
    "        or state[1] > self.map_size[1]-1:\n",
    "            state = self.prev_state\n",
    "        #e-greedy policy\n",
    "                    \n",
    "        self.p = np.random.uniform()\n",
    "        if self.p < self.epsillon:\n",
    "            self.action = self.action_space.sample()\n",
    "        else:\n",
    "            self.action = np.argmax(self.Q, axis=0)[state[0],state[1]]\n",
    "        \n",
    "        \n",
    "        \n",
    "        return self.action\n",
    "        \n",
    "    def render(self,episode,step,state):\n",
    "        # 시각화\n",
    "        plt.ion()\n",
    "        plt.title(\"Grid World\")\n",
    "        plt.figtext(self.text_pos_x,self.text_pos_y, f\"Episode = {episode}\")\n",
    "        plt.figtext(self.text_pos_x,self.text_pos_y-0.1, \"Step : {}\".format(step))\n",
    "        \n",
    "        \n",
    "        self.world[state[0],state[1]] = -1\n",
    "        self.world[self.goal[0],self.goal[1]] = 3\n",
    "        plt.matshow(self.world,fignum=0)\n",
    "        plt.draw()\n",
    "        plt.pause(0.01) #\n",
    "        plt.clf()\n",
    "        self.world[state[0],state[1]] = 0\n",
    "        \n",
    "        for obs_x,obs_y in self.obstacle:\n",
    "            self.world[obs_x,obs_y] = 2\n",
    "        \n",
    "    def render_text(self,obs,reward):\n",
    "        # 시뮬레이션 정보 출력\n",
    "        plt.figtext(self.text_pos_x,self.text_pos_y-0.3, f\"Reward : {reward}\")\n",
    "#         plt.figtext(self.text_pos_x-0.05,self.text_pos_y-0.4, f\"State Value Function : \\n{np.around(self.V,4)}\")\n",
    "#         plt.figtext(self.text_pos_x+0.1,self.text_pos_y-0.6, f\"Action Value Function : \\n{np.around(self.Q,4)}\")\n",
    "        plt.figtext(self.text_pos_x-0.05,self.text_pos_y-0.6, f\"Optimal Policy : \\n{np.argmax(self.Q, axis=0)}\")\n",
    "        plt.figtext(self.text_pos_x,self.text_pos_y-0.15, f\"Eps : \\n{self.epsillon}\")\n",
    "        if obs == 2:\n",
    "            plt.figtext(self.text_pos_x,self.text_pos_y-0.2, \"None\")\n",
    "        elif obs == 1:\n",
    "            plt.figtext(self.text_pos_x,self.text_pos_y-0.2, \"GOAL IN\")\n",
    "        elif obs == 0:\n",
    "            plt.figtext(self.text_pos_x,self.text_pos_y-0.2, \"Obstacle\")\n",
    "        elif obs == 3:\n",
    "            plt.figtext(self.text_pos_x,self.text_pos_y-0.2, \"Out of map\")\n",
    "        \n",
    "            \n",
    "            \n",
    "    def get_reward(self):\n",
    "        # 리워드 설정\n",
    "        if self.agent_pos in self.obstacle:\n",
    "            self.reward = -1\n",
    "            return self.reward\n",
    "        \n",
    "        elif self.agent_pos == self.goal:\n",
    "            self.reward = 1\n",
    "            return self.reward\n",
    "        \n",
    "        # 맵 밖으로 나갔을 시 \n",
    "        elif self.agent_pos[0] < 0 \\\n",
    "        or self.agent_pos[1] < 0 \\\n",
    "        or self.agent_pos[0] > self.map_size[0]-1 \\\n",
    "        or self.agent_pos[1] > self.map_size[1]-1:\n",
    "            self.reward = -1\n",
    "            return self.reward\n",
    "        \n",
    "        else:\n",
    "            self.reward = 0\n",
    "            return self.reward\n",
    "        \n",
    "    def value_function_update(self,action,next_action,state,next_state):\n",
    "        #State value function\n",
    "        try:\n",
    "            self.V[state[0]][state[1]] = \\\n",
    "            self.V[state[0]][state[1]] + self.alpha * (self.reward + self.gamma*self.V[next_state[0]][next_state[1]]- self.V[state[0]][state[1]])\n",
    "        except IndexError: # Agent가 밖으로 나갔을 시 Value function update 예외 처리\n",
    "            self.V[state[0]][state[1]] = \\\n",
    "        self.V[state[0]][state[1]] + self.alpha * (self.reward + self.gamma*self.V[state[0]][state[1]]- self.V[state[0]][state[1]])\n",
    "        #Action value function\n",
    "        try:\n",
    "            self.Q[action,state[0],state[1]] = \\\n",
    "            self.Q[action,state[0],state[1]] + self.alpha*(self.reward + self.gamma * self.Q[next_action,next_state[0],next_state[1]]-self.Q[action,state[0],state[1]])\n",
    "        except IndexError:\n",
    "            self.Q[action,state[0],state[1]] = \\\n",
    "            self.Q[action,state[0],state[1]] + self.alpha*(self.reward + self.gamma * self.Q[action,state[0],state[1]]-self.Q[action,state[0],state[1]])\n",
    "        \n",
    "    def close(self):\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = GridEnv()  #np.argmax(self.Q, axis=0)[state[0],state[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4f812fd288a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 에피소드 수 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#환경 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# step 상한선 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-51b9d24d5233>\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m#에이전트의 초기위치 및 장애물 위치 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobstacle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobstacle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobstacle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobstacle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for episode in range(10000): # 에피소드 수 설정\n",
    "    #환경 생성\n",
    "    obs,reward = env.reset()\n",
    "    \n",
    "    for step in range(100): # step 상한선 설정\n",
    "        if episode % 100 ==0:\n",
    "            env.render(episode, step,0.1)\n",
    "            env.render_text(obs,reward)\n",
    "       \n",
    "        \n",
    "        #Sampling Action\n",
    "        action = env.policy(env.agent_pos)\n",
    "        \n",
    "        #Take action -> Observe Reward, Observe Next State\n",
    "        obs, reward, is_done , next_state, state = env.step(action)\n",
    "        \n",
    "    \n",
    "        #Choose next action from next state\n",
    "        next_action = env.policy(next_state)\n",
    "        \n",
    "        #Value function update\n",
    "        env.value_function_update(action,next_action,state,next_state)\n",
    "        \n",
    "      \n",
    "            \n",
    "        if is_done: \n",
    "            \n",
    "            if env.epsillon > 0.1:\n",
    "                env.epsillon -= 0.001\n",
    "            else:\n",
    "                env.epsilon = 0.1\n",
    "            \n",
    "            break;\n",
    "env.close()\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
