{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.8 (default, Feb 24 2021, 21:46:12) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "#Jupyter notebook 에서 pop up window\n",
    "%matplotlib tk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open AI Gym을 이용한 Custom Environment 생성 예\n",
    "class GridEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 초기 환경 구성 시 필요한 파라미터 설정\n",
    "        self.map_size = (12,12)\n",
    "        '''\n",
    "        oooooooooooo\n",
    "        xxxxxxxxxxxo\n",
    "        oooooooooooo\n",
    "        oxxxxxxxxxxx\n",
    "        oooooooooooo\n",
    "        xxxxxxxxxxxo\n",
    "        oooooooooooo\n",
    "        oxxxxxxxxxxx\n",
    "        oooooooooooo\n",
    "        xxxxxxxxxxxo\n",
    "        oooooooooooo\n",
    "        oxxxxxxxxxxx\n",
    "        left bottom corner is (0, 0). positive x follows right. positive y follows upward\n",
    "        \n",
    "        highly hardcoded\n",
    "        '''\n",
    "        self.agent_pos = [0,0]\n",
    "        self.obstacle = []\n",
    "        self.obstacle.extend((0, x) for x in range(1,12))\n",
    "        self.obstacle.extend((2, x) for x in range(0,11))\n",
    "        self.obstacle.extend((4, x) for x in range(1,12))\n",
    "        self.obstacle.extend((6, x) for x in range(0,11))\n",
    "        self.obstacle.extend((8, x) for x in range(1,12))\n",
    "        self.obstacle.extend((10, x) for x in range(0,11))\n",
    "                        \n",
    "        self.goal = [11,0]#[self.map_size[0]-1,self.map_size[1]-1]\n",
    "        self.is_done = None\n",
    "        # Temporal Difference \n",
    "        self.V = np.zeros(self.map_size)\n",
    "        self.Q = np.zeros([4,*self.map_size])\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.1\n",
    "        self.epsillon = 1\n",
    "       \n",
    "        # Open AI gym 환경 정보 설정\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.obs_space = gym.spaces.Discrete(3)\n",
    "        \n",
    "        #plt figtext 위치\n",
    "        self.text_pos_x = 0.8\n",
    "        self.text_pos_y = 0.9\n",
    "\n",
    "    def step(self, action):\n",
    "        self.prev_state = copy.deepcopy(self.agent_pos)\n",
    "        if action == 0: #Up\n",
    "            self.agent_pos[0] += -1\n",
    "        elif action == 1: #Down\n",
    "            self.agent_pos[0] += +1\n",
    "        elif action == 2: #Left\n",
    "            self.agent_pos[1] += -1\n",
    "        elif action == 3: #Right\n",
    "            self.agent_pos[1] += +1\n",
    "        else:\n",
    "            raise Exception(\"Action is not defined\")\n",
    "      \n",
    "    \n",
    "        self.next_state = copy.deepcopy(self.agent_pos)\n",
    "        \n",
    "        #self.render_text(self.obs(),self.get_reward())\n",
    "            \n",
    "        return self.obs(), self.get_reward(), self._is_done(), self.next_state, self.prev_state\n",
    "    \n",
    "    def obs(self):\n",
    "        \n",
    "        if self.agent_pos in self.obstacle: # obstacle \n",
    "            return -1\n",
    "        elif self.agent_pos[0] < 0 \\\n",
    "        or self.agent_pos[1] < 0 \\\n",
    "        or self.agent_pos[0] > self.map_size[0]-1 \\\n",
    "        or self.agent_pos[1] > self.map_size[1]-1:\n",
    "            return -1\n",
    "        elif self.agent_pos == self.goal: # goal reached\n",
    "            return 1\n",
    "        else: # nothing special\n",
    "            return 2\n",
    "    \n",
    "    def _is_done(self):\n",
    "        # 맵 밖으로 나갔을 시\n",
    "        if self.agent_pos[0] < 0 \\\n",
    "        or self.agent_pos[1] < 0 \\\n",
    "        or self.agent_pos[0] > self.map_size[0]-1 \\\n",
    "        or self.agent_pos[1] > self.map_size[1]-1:\n",
    "            #print(\"Out of map\")\n",
    "            return True\n",
    "            # 도착 지점 도착시\n",
    "        elif self.agent_pos == self.goal:\n",
    "            #print(\"Goal in\")\n",
    "            return True\n",
    "        elif tuple(self.agent_pos) in self.obstacle:\n",
    "            #print(\"Obstacle\")\n",
    "            return True\n",
    "        else:\n",
    "            #print(\"Usual\")\n",
    "            return False\n",
    "            \n",
    "    def reset(self):\n",
    "        '''환경 초기화'''\n",
    "        \n",
    "        #맵 사이즈 설정\n",
    "        self.agent_pos = [0,0]\n",
    "        self.world = np.zeros(self.map_size)        \n",
    "        \n",
    "        #에이전트의 초기위치 및 장애물 위치 설정\n",
    "        for obs_x,obs_y in self.obstacle:\n",
    "            self.world[obs_x,obs_y] = 2\n",
    "        \n",
    "        \n",
    "        #[np.random.randint(self.map_size[0]),np.random.randint(self.map_size[1])]\n",
    "\n",
    "        return self.obs(),self.get_reward(),self.agent_pos\n",
    "    \n",
    "    def policy(self, state = None): \n",
    "        if state[0] < 0 \\\n",
    "        or state[1] < 0 \\\n",
    "        or state[0] > self.map_size[0]-1 \\\n",
    "        or state[1] > self.map_size[1]-1:\n",
    "            state = self.prev_state\n",
    "        #e-greedy policy\n",
    "                    \n",
    "        self.p = np.random.uniform()\n",
    "        if self.p < self.epsillon:\n",
    "            self.action = self.action_space.sample()\n",
    "        else:\n",
    "            self.action = np.argmax(self.Q, axis=0)[state[0],state[1]]\n",
    "        \n",
    "        \n",
    "        \n",
    "        return self.action\n",
    "        \n",
    "    def render(self,episode,step,state,dt):\n",
    "        # 시각화\n",
    "        plt.ion()\n",
    "        plt.title(\"Grid World\")\n",
    "        plt.figtext(self.text_pos_x,self.text_pos_y, f\"Episode = {episode}\")\n",
    "        plt.figtext(self.text_pos_x,self.text_pos_y-0.1, \"Step : {}\".format(step))\n",
    "        \n",
    "        \n",
    "        self.world[state[0],state[1]] = -1\n",
    "        self.world[self.goal[0],self.goal[1]] = 3\n",
    "        plt.matshow(self.world,fignum=0)\n",
    "        plt.draw()\n",
    "        plt.pause(dt) #\n",
    "        plt.clf()\n",
    "        self.world[state[0],state[1]] = 0\n",
    "        \n",
    "        for obs_x,obs_y in self.obstacle:\n",
    "            self.world[obs_x,obs_y] = 2\n",
    "        \n",
    "    def render_text(self,obs,reward):\n",
    "        # 시뮬레이션 정보 출력va.desktop/java.awt.EventDispatchThread.run(EventDispatchThread.java:90)\n",
    "\n",
    "\n",
    "        plt.figtext(self.text_pos_x,self.text_pos_y-0.3, f\"Reward : {reward}\")\n",
    "#         plt.figtext(self.text_pos_x-0.05,self.text_pos_y-0.4, f\"State Value Function : \\n{np.around(self.V,4)}\")\n",
    "#         plt.figtext(self.text_pos_x+0.1,self.text_pos_y-0.6, f\"Action Value Function : \\n{np.around(self.Q,4)}\")\n",
    "        plt.figtext(self.text_pos_x-0.05,self.text_pos_y-0.6, f\"Optimal Policy : \\n{np.argmax(self.Q, axis=0)}\")\n",
    "        plt.figtext(self.text_pos_x,self.text_pos_y-0.15, f\"Eps : \\n{self.epsillon}\")\n",
    "        if obs == 2:\n",
    "            plt.figtext(self.text_pos_x,self.text_pos_y-0.2, \"None\")\n",
    "        elif obs == 1:\n",
    "            plt.figtext(self.text_pos_x,self.text_pos_y-0.2, \"GOAL IN\")\n",
    "        elif obs == 0:\n",
    "            plt.figtext(self.text_pos_x,self.text_pos_y-0.2, \"Obstacle\")\n",
    "        elif obs == 3:\n",
    "            plt.figtext(self.text_pos_x,self.text_pos_y-0.2, \"Out of map\")\n",
    "        \n",
    "            \n",
    "            \n",
    "    def get_reward(self):\n",
    "        # 리워드 설정\n",
    "        if tuple(self.agent_pos) in self.obstacle:\n",
    "            self.reward = -1\n",
    "            return self.reward\n",
    "        elif self.agent_pos == self.goal:\n",
    "            self.reward = 100\n",
    "            return self.reward\n",
    "        # 맵 밖으로 나갔을 시 \n",
    "        elif self.agent_pos[0] < 0 \\\n",
    "        or self.agent_pos[1] < 0 \\\n",
    "        or self.agent_pos[0] > self.map_size[0]-1 \\\n",
    "        or self.agent_pos[1] > self.map_size[1]-1:\n",
    "            self.reward = -1\n",
    "            return self.reward\n",
    "        else:\n",
    "            self.reward = 0\n",
    "            return self.reward\n",
    "        \n",
    "    def value_function_update(self,action,next_action,state,next_state):\n",
    "        #State value function\n",
    "        try:\n",
    "            self.V[state[0]][state[1]] = \\\n",
    "            self.V[state[0]][state[1]] + self.alpha * (self.reward + self.gamma*self.V[next_state[0]][next_state[1]]- self.V[state[0]][state[1]])\n",
    "        except IndexError: # Agent가 밖으로 나갔을 시 Value function update 예외 처리\n",
    "            self.V[state[0]][state[1]] = \\\n",
    "        self.V[state[0]][state[1]] + self.alpha * (self.reward + self.gamma*self.V[state[0]][state[1]]- self.V[state[0]][state[1]])\n",
    "        #Action value function\n",
    "   \n",
    "        try:\n",
    "            self.Q[action,state[0],state[1]] = \\\n",
    "            self.Q[action,state[0],state[1]] + self.alpha*(self.reward + self.gamma * np.max(self.Q[:,next_state[0],next_state[1]])-self.Q[action,state[0],state[1]])\n",
    "        except IndexError:\n",
    "            self.Q[action,state[0],state[1]] = \\\n",
    "            self.Q[action,state[0],state[1]] + self.alpha*(self.reward + self.gamma * np.max(self.Q[action,state[0],state[1]])-self.Q[action,state[0],state[1]])\n",
    "        \n",
    "    def close(self):\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = GridEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# action = env.policy(env.agent_pos)\n",
    "# obs, reward, is_done , next_state, state = env.step(action)\n",
    "# print(obs, reward, is_done, state,next_state)\n",
    "# next_action = env.policy(next_state)\n",
    "# env.value_function_update(action,next_action,state,next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1001/100000 [00:01<01:54, 862.01it/s]"
     ]
    }
   ],
   "source": [
    "for episode in tqdm(range(100000)): # 에피소드 수 설정\n",
    "    #환경 생성\n",
    "    obs,reward,state = env.reset()\n",
    "    \n",
    "    for step in range(500): # step 상한선 설정\n",
    "        \n",
    "        \n",
    "        #Sampling Action\n",
    "        action = env.policy(env.agent_pos)\n",
    "        \n",
    "        #Take action -> Observe Reward, Observe Next State\n",
    "        obs, reward, is_done , next_state, state = env.step(action)\n",
    "    \n",
    "        #Choose next action from next state\n",
    "        next_action = env.policy(next_state)\n",
    "        \n",
    "        #Value function update\n",
    "        env.value_function_update(action,next_action,state,next_state)\n",
    "        \n",
    "        if episode % 500 == 0 and episode > 0:\n",
    "            env.render(episode, step,state,0.1)\n",
    "            env.render_text(obs,reward)\n",
    "            #print('up down left right')\n",
    "            #print(env.Q[:, 0, 0])\n",
    "            #print(env.Q[:, 1, :5])\n",
    "            #input('')\n",
    "\n",
    "        if is_done: \n",
    "            if env.epsillon > 0.0:\n",
    "                env.epsillon -= 1/100000\n",
    "            else:\n",
    "                env.epsilon = 0\n",
    "            break\n",
    "env.close()\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
