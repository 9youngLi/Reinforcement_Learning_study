{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "#Jupyter notebook 에서 pop up window\n",
    "%matplotlib tk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open AI Gym을 이용한 Custom Environment 생성 예\n",
    "class GridEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 초기 환경 구성 시 필요한 파라미터 설정\n",
    "        self.map_size = (3,3)\n",
    "        self.agent_pos = [0,0]\n",
    "        self.obstacle = [[1,1],[1,2]]\n",
    "        self.goal = [2,2]\n",
    "        # Temporal Difference \n",
    "        self.V = np.zeros([3,3])\n",
    "        self.Q = np.zeros([3,3])\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.1\n",
    "        \n",
    "       \n",
    "        print(f\"Value function \\n {self.V}\\n\")\n",
    "        print(f\"Action Value function \\n {self.Q}\\n\")\n",
    "        print(f\"Initial Policy : Random\")\n",
    "        # Open AI gym 환경 정보 설정\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.obs_space = gym.spaces.Discrete(3)\n",
    "        \n",
    "        #plt figtext 위치\n",
    "        self.text_pos_x = 0.8\n",
    "        self.text_pos_y = 0.9\n",
    "\n",
    "    def step(self, action):\n",
    "        self.post_state = copy.deepcopy(self.agent_pos)\n",
    "        if action == 0: #Left\n",
    "            self.agent_pos[0] += -1\n",
    "        elif action == 1: #Right\n",
    "            self.agent_pos[0] += +1\n",
    "        elif action == 2: #Up\n",
    "            self.agent_pos[1] += -1\n",
    "        elif action == 3: #Down\n",
    "            self.agent_pos[1] += +1\n",
    "        else:\n",
    "            raise Exception(\"Action is not defined\")\n",
    "        \n",
    "#         if self.agent_pos[0] < 0 \\\n",
    "#         or self.agent_pos[1] < 0 \\\n",
    "#         or self.agent_pos[0] > self.map_size[0]-1 \\\n",
    "#         or self.agent_pos[1] > self.map_size[1]-1:\n",
    "#             if action == 0: #Left\n",
    "#                 self.agent_pos[0] += +1\n",
    "#             elif action == 1: #Right\n",
    "#                 self.agent_pos[0] += -1\n",
    "#             elif action == 2: #Up\n",
    "#                 self.agent_pos[1] += +1\n",
    "#             elif action == 3: #Down\n",
    "#                 self.agent_pos[1] += -1\n",
    "            \n",
    "            \n",
    "        return self.obs(), self.get_reward(), self._is_done(), self.agent_pos, self.post_state\n",
    "    \n",
    "    def obs(self):\n",
    "        \n",
    "        if self.agent_pos in self.obstacle:\n",
    "            return 0\n",
    "        \n",
    "        elif self.agent_pos == self.goal:    \n",
    "            return 1\n",
    "        elif self.agent_pos[0] < 0 \\\n",
    "        or self.agent_pos[1] < 0 \\\n",
    "        or self.agent_pos[0] > self.map_size[0]-1 \\\n",
    "        or self.agent_pos[1] > self.map_size[1]-1:\n",
    "            return 3\n",
    "        else:    \n",
    "            return 2\n",
    "    \n",
    "    def _is_done(self):\n",
    "        # 맵 밖으로 나갔을 시 \n",
    "        if self.agent_pos[0] < 0 \\\n",
    "        or self.agent_pos[1] < 0 \\\n",
    "        or self.agent_pos[0] > self.map_size[0]-1 \\\n",
    "        or self.agent_pos[1] > self.map_size[1]-1:\n",
    "            return True\n",
    "        # 도착 지점 도착시\n",
    "        elif self.agent_pos == self.goal:\n",
    "            return True\n",
    "        \n",
    "        else:\n",
    "            return False\n",
    "            \n",
    "    def reset(self):\n",
    "        '''환경 초기화'''\n",
    "        \n",
    "        #맵 사이즈 설정\n",
    "        self.world = np.zeros(self.map_size)        \n",
    "        \n",
    "        #에이전트의 초기위치 및 장애물 위치 설정\n",
    "        self.world[self.obstacle[0][0],self.obstacle[0][1]] = 2\n",
    "        self.world[self.obstacle[1][0],self.obstacle[1][1]] = 2\n",
    "        \n",
    "        \n",
    "        self.agent_pos = [0,2]\n",
    "\n",
    "        return self.obs()\n",
    "    def policy(self, state = None): \n",
    "        if state == None:\n",
    "            \n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            \n",
    "            return env.action_space.sample()\n",
    "        \n",
    "    def render(self,episode,step):\n",
    "        # 시각화\n",
    "        plt.ion()\n",
    "        plt.title(\"Grid World\")\n",
    "        plt.figtext(self.text_pos_x,self.text_pos_y, f\"Episode = {episode}\")\n",
    "        plt.figtext(self.text_pos_x,self.text_pos_y-0.1, \"Step : {}\".format(step))\n",
    "        \n",
    "        self.world[self.agent_pos[0], self.agent_pos[1]] = -1\n",
    "        self.world[self.goal[0],self.goal[1]] = 3\n",
    "        plt.matshow(self.world,fignum=0)\n",
    "        plt.draw()\n",
    "        plt.pause(0.01) #\n",
    "        plt.clf()\n",
    "        self.world[self.agent_pos[0], self.agent_pos[1]] = 0\n",
    "        \n",
    "        self.world[self.obstacle[0][0],self.obstacle[0][1]] = 2\n",
    "        self.world[self.obstacle[1][0],self.obstacle[1][1]] = 2\n",
    "        \n",
    "    def render_text(self,obs,reward):\n",
    "        # 시뮬레이션 정보 출력\n",
    "        plt.figtext(self.text_pos_x,self.text_pos_y-0.3, f\"Reward : {reward}\")\n",
    "        if obs == 2:\n",
    "            plt.figtext(self.text_pos_x,self.text_pos_y-0.2, \"\")\n",
    "        elif obs == 1:\n",
    "            plt.figtext(self.text_pos_x,self.text_pos_y-0.2, \"GOAL IN\")\n",
    "        elif obs == 0:\n",
    "            plt.figtext(self.text_pos_x,self.text_pos_y-0.2, \"Obstacle\")\n",
    "        elif obs == 3:\n",
    "            plt.figtext(self.text_pos_x,self.text_pos_y-0.2, \"Out of map\")\n",
    "            \n",
    "    def get_reward(self):\n",
    "        # 리워드 설정\n",
    "        if self.agent_pos in self.obstacle:\n",
    "            return -1\n",
    "        \n",
    "        elif self.agent_pos == self.goal:\n",
    "            return +1\n",
    "        \n",
    "        elif self.agent_pos[0] < 0 \\\n",
    "        or self.agent_pos[1] < 0 \\\n",
    "        or self.agent_pos[0] > self.map_size[0]-1 \\\n",
    "        or self.agent_pos[1] > self.map_size[1]-1:\n",
    "            return -1\n",
    "        \n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    \n",
    "    def close(self):\n",
    "        # Clear env\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function \n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "Action Value function \n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "Initial Policy : Random\n"
     ]
    }
   ],
   "source": [
    "env = GridEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for episode in range(100): # 에피소드 수 설정\n",
    "    #환경 생성\n",
    "    obs = env.reset()\n",
    "    for step in range(100): # step 상한선 설정\n",
    "        \n",
    "        env.render(episode, step)\n",
    "        #Sampling Action\n",
    "        action = env.policy(env.agent_pos)\n",
    "        \n",
    "        #Take action -> Observe Reward, Observe Next State\n",
    "        obs, reward, is_done , next_state, state = env.step(action)\n",
    "        env.render_text(obs,reward)        \n",
    "        \n",
    "        #Value function update\n",
    "        try:\n",
    "            env.V[state[0]][state[1]] = \\\n",
    "            env.V[state[0]][state[1]] + env.alpha * (reward + env.gamma*env.V[next_state[0]][next_state[1]]- env.V[state[0]][state[1]])\n",
    "        except IndexError: # Agent가 밖으로 나갔을 시 Value function update 예외 처리\n",
    "            env.V[state[0]][state[1]] = \\\n",
    "        env.V[state[0]][state[1]] + env.alpha * (reward + env.gamma*env.V[state[0]][state[1]]- env.V[state[0]][state[1]])\n",
    "        \n",
    "            \n",
    "        if is_done:                \n",
    "            break;\n",
    "env.close()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
