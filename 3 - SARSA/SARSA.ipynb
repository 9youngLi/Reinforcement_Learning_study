{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "#Jupyter notebook 에서 pop up window\n",
    "%matplotlib tk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open AI Gym을 이용한 Custom Environment 생성 예\n",
    "class GridEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 초기 환경 구성 시 필요한 파라미터 설정\n",
    "        self.map_size = (3,3)\n",
    "        self.agent_pos = [0,0]\n",
    "        self.obstacle = [[1,1],[1,2]]\n",
    "        self.goal = [2,2]\n",
    "        # Temporal Difference \n",
    "        self.V = np.zeros([3,3])\n",
    "        self.Q = np.zeros([4,3,3])\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.1\n",
    "        \n",
    "       \n",
    "        print(f\"Value function \\n {self.V}\\n\")\n",
    "        print(f\"Action Value function \\n {self.Q}\\n\")\n",
    "        print(f\"Initial Policy : Random\")\n",
    "        # Open AI gym 환경 정보 설정\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.obs_space = gym.spaces.Discrete(3)\n",
    "        \n",
    "        #plt figtext 위치\n",
    "        self.text_pos_x = 0.8\n",
    "        self.text_pos_y = 0.9\n",
    "\n",
    "    def step(self, action):\n",
    "        self.prev_state = copy.deepcopy(self.agent_pos)\n",
    "        if action == 0: #Left\n",
    "            self.agent_pos[0] += -1\n",
    "        elif action == 1: #Right\n",
    "            self.agent_pos[0] += +1\n",
    "        elif action == 2: #Up\n",
    "            self.agent_pos[1] += -1\n",
    "        elif action == 3: #Down\n",
    "            self.agent_pos[1] += +1\n",
    "        else:\n",
    "            raise Exception(\"Action is not defined\")\n",
    "        self.next_state = copy.deepcopy(self.agent_pos)\n",
    "        \n",
    "        self.render_text(self.obs(),self.get_reward())\n",
    "            \n",
    "        return self.obs(), self.get_reward(), self._is_done(), self.next_state, self.prev_state\n",
    "    \n",
    "    def obs(self):\n",
    "        \n",
    "        if self.agent_pos in self.obstacle:\n",
    "            return 0\n",
    "        \n",
    "        elif self.agent_pos == self.goal:    \n",
    "            return 1\n",
    "        elif self.agent_pos[0] < 0 \\\n",
    "        or self.agent_pos[1] < 0 \\\n",
    "        or self.agent_pos[0] > self.map_size[0]-1 \\\n",
    "        or self.agent_pos[1] > self.map_size[1]-1:\n",
    "            return 3\n",
    "        else:    \n",
    "            return 2\n",
    "    \n",
    "    def _is_done(self):\n",
    "        # 맵 밖으로 나갔을 시 \n",
    "        if self.agent_pos[0] < 0 \\\n",
    "        or self.agent_pos[1] < 0 \\\n",
    "        or self.agent_pos[0] > self.map_size[0]-1 \\\n",
    "        or self.agent_pos[1] > self.map_size[1]-1:\n",
    "            return True\n",
    "        # 도착 지점 도착시\n",
    "        elif self.agent_pos == self.goal:\n",
    "            return True\n",
    "        \n",
    "        else:\n",
    "            return False\n",
    "            \n",
    "    def reset(self):\n",
    "        '''환경 초기화'''\n",
    "        \n",
    "        #맵 사이즈 설정\n",
    "        self.world = np.zeros(self.map_size)        \n",
    "        \n",
    "        #에이전트의 초기위치 및 장애물 위치 설정\n",
    "        self.world[self.obstacle[0][0],self.obstacle[0][1]] = 2\n",
    "        self.world[self.obstacle[1][0],self.obstacle[1][1]] = 2\n",
    "        \n",
    "        \n",
    "        self.agent_pos = [0,0]\n",
    "\n",
    "        return self.obs()\n",
    "    \n",
    "    def policy(self, state = None): \n",
    "        #e-greedy policy\n",
    "        self.policy_table = np.argmax(self.Q, axis=0)\n",
    "        return self.policy_table[state[0],state[1]]\n",
    "        \n",
    "    def render(self,episode,step):\n",
    "        # 시각화\n",
    "        plt.ion()\n",
    "        plt.title(\"Grid World\")\n",
    "        plt.figtext(self.text_pos_x,self.text_pos_y, f\"Episode = {episode}\")\n",
    "        plt.figtext(self.text_pos_x,self.text_pos_y-0.1, \"Step : {}\".format(step))\n",
    "        \n",
    "        self.world[self.agent_pos[0], self.agent_pos[1]] = -1\n",
    "        self.world[self.goal[0],self.goal[1]] = 3\n",
    "        plt.matshow(self.world,fignum=0)\n",
    "        plt.draw()\n",
    "        plt.pause(0.001) #\n",
    "        plt.clf()\n",
    "        self.world[self.agent_pos[0], self.agent_pos[1]] = 0\n",
    "        \n",
    "        self.world[self.obstacle[0][0],self.obstacle[0][1]] = 2\n",
    "        self.world[self.obstacle[1][0],self.obstacle[1][1]] = 2\n",
    "        \n",
    "    def render_text(self,obs,reward):\n",
    "        # 시뮬레이션 정보 출력\n",
    "        plt.figtext(self.text_pos_x,self.text_pos_y-0.3, f\"Reward : {reward}\")\n",
    "        plt.figtext(self.text_pos_x-0.05,self.text_pos_y-0.4, f\"State Value Function : \\n{np.around(self.V,4)}\")\n",
    "        plt.figtext(self.text_pos_x+0.1,self.text_pos_y-0.6, f\"Action Value Function : \\n{np.around(self.Q,4)}\")\n",
    "        if obs == 2:\n",
    "            plt.figtext(self.text_pos_x,self.text_pos_y-0.2, \"\")\n",
    "        elif obs == 1:\n",
    "            plt.figtext(self.text_pos_x,self.text_pos_y-0.2, \"GOAL IN\")\n",
    "        elif obs == 0:\n",
    "            plt.figtext(self.text_pos_x,self.text_pos_y-0.2, \"Obstacle\")\n",
    "        elif obs == 3:\n",
    "            plt.figtext(self.text_pos_x,self.text_pos_y-0.2, \"Out of map\")\n",
    "            \n",
    "            \n",
    "    def get_reward(self):\n",
    "        # 리워드 설정\n",
    "        if self.agent_pos in self.obstacle:\n",
    "            self.reward = -1\n",
    "            return self.reward\n",
    "        \n",
    "        elif self.agent_pos == self.goal:\n",
    "            self.reward = 1\n",
    "            return self.reward\n",
    "        \n",
    "        # 맵 밖으로 나갔을 시 \n",
    "        elif self.agent_pos[0] < 0 \\\n",
    "        or self.agent_pos[1] < 0 \\\n",
    "        or self.agent_pos[0] > self.map_size[0]-1 \\\n",
    "        or self.agent_pos[1] > self.map_size[1]-1:\n",
    "            self.reward = -1\n",
    "            return self.reward\n",
    "        \n",
    "        else:\n",
    "            self.reward = 0\n",
    "            return self.reward\n",
    "        \n",
    "    def value_function_update(self,action,next_action,state,next_state):\n",
    "        #State value function\n",
    "        try:\n",
    "            self.V[state[0]][state[1]] = \\\n",
    "            self.V[state[0]][state[1]] + self.alpha * (self.reward + self.gamma*self.V[next_state[0]][next_state[1]]- self.V[state[0]][state[1]])\n",
    "        except IndexError: # Agent가 밖으로 나갔을 시 Value function update 예외 처리\n",
    "            self.V[state[0]][state[1]] = \\\n",
    "        self.V[state[0]][state[1]] + self.alpha * (self.reward + self.gamma*self.V[state[0]][state[1]]- self.V[state[0]][state[1]])\n",
    "        #Action value function\n",
    "        try:\n",
    "            self.Q[action,state[0],state[1]] = \\\n",
    "            self.Q[action,state[0],state[1]] + self.gamma*(self.reward + self.gamma * self.Q[next_action,next_state[0],next_state[1]]-self.Q[action,state[0],state[1]])\n",
    "        except IndexError:\n",
    "            self.Q[action,state[0],state[1]] = \\\n",
    "            self.Q[action,state[0],state[1]] + self.gamma*(self.reward + self.gamma * self.Q[action,state[0],state[1]]-self.Q[action,state[0],state[1]])\n",
    "        \n",
    "    def close(self):\n",
    "        # Clear env\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function \n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "Action Value function \n",
      " [[[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]]\n",
      "\n",
      "Initial Policy : Random\n"
     ]
    }
   ],
   "source": [
    "env = GridEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for episode in range(100): # 에피소드 수 설정\n",
    "    #환경 생성\n",
    "    obs = env.reset()\n",
    "    for step in range(100): # step 상한선 설정\n",
    "        \n",
    "        env.render(episode, step)\n",
    "        #Sampling Action\n",
    "        action = env.policy(env.agent_pos)\n",
    "        \n",
    "        #Take action -> Observe Reward, Observe Next State\n",
    "        obs, reward, is_done , next_state, state = env.step(action)\n",
    "     \n",
    "        #Choose next action from next state\n",
    "        next_action = env.policy(next_state)\n",
    "    \n",
    "        #Value function update\n",
    "        env.value_function_update(action,next_action,state,next_state)\n",
    "        \n",
    "        \n",
    "            \n",
    "        if is_done:                \n",
    "            break;\n",
    "env.close()\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-3.11035971, -3.48402186, -0.99      ],\n",
       "        [-1.25334463, -2.8946638 , -1.39239   ],\n",
       "        [-0.87329418, -1.65100348,  0.        ]],\n",
       "\n",
       "       [[-0.99171261, -0.67816047, -0.9       ],\n",
       "        [ 0.45823016, -0.70041256,  0.999     ],\n",
       "        [-2.46429   , -3.1425039 ,  0.        ]],\n",
       "\n",
       "       [[-1.84445188, -1.91301383, -1.30809973],\n",
       "        [-2.07122366,  0.40176471, -2.84874244],\n",
       "        [-0.99      , -0.8019    ,  0.        ]],\n",
       "\n",
       "       [[-1.45672031, -0.88226715, -2.46429   ],\n",
       "        [-1.6307603 , -1.97204374, -0.9       ],\n",
       "        [-2.47916206,  0.999     ,  0.        ]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [1, 2, 1],\n",
       "       [0, 3, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(env.Q, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
